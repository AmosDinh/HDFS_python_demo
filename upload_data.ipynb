{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyHDFS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jingw/pyhdfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-The methods and return values generally map directly to WebHDFS endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhdfs\n",
    "fs = pyhdfs.HdfsClient(hosts='nn1.example.com:50070,nn2.example.com:50070', user_name='someone')\n",
    "fs.list_status('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.listdir('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def recurse_copy_to_remote(fs:pyhdfs.HdfsClient, local_path, remote_path):\n",
    "    for f in os.listdir(local_path):\n",
    "        try:\n",
    "            if os.path.isdir(local_path/f):\n",
    "                # create remote directory\n",
    "                fs.mkdirs(remote_path/f)\n",
    "                recurse_copy_to_remote(fs, local_path/f, remote_path/f)\n",
    "            else:\n",
    "                fs.copy_from_local(local_path/f, remote_path/f)\n",
    "        except pyhdfs.HdfsException as e:\n",
    "            # HdfsFileAlreadyExistsException\n",
    "            # HdfsFileNotFoundException\n",
    "            # HdfsHttpException\n",
    "            # HdfsIOException\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "recurse_copy_to_remote(fs, Path('data'),Path('/mushrooms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.mkdirs('/fruit/x/y') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.create('/fruit/apple', 'delicious')\n",
    "fs.append('/fruit/apple', ' food')\n",
    "list(fs.walk('/fruit'))\n",
    "fs.delete(dir, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.create(file_path) as f:\n",
    "    f.write(b'Hello, world!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.closing(fs.open('/fruit/apple')) as f:\n",
    "    f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## native RPC client interfaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- better throughput than WebHDFS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libhdfs\n",
    "- c wrapper for the HDFS Java Client\n",
    "- supported by major Hadoop vendors, part of the Apache Hadoop project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arrow.apache.org/docs/python/generated/pyarrow.fs.HadoopFileSystem.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wesmckinney.com/blog/python-hdfs-interfaces/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local.create_dir('/tmp/new_folder')\n",
    "# local.copy_file('/tmp/local_fs.dat', '/tmp/new_folder/local_fs.dat')\n",
    "# local.get_file_info('/tmp/new_folder/local_fs.dat')\n",
    "# local.delete_dir_contents('/tmp/new_folder')\n",
    "# local.get_file_info('/tmp/new_folder')\n",
    "# local.get_file_info('/tmp/new_folder/local_fs.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(hdfs_filesystem, filepath):\n",
    "    local_file = filepath\n",
    "    hdfs_file = '/tmp/' + os.path.basename(filepath)\n",
    "    hdfs_filesystem.c(local_file, hdfs_file)\n",
    "    print('Uploaded file to HDFS: ' + hdfs_file)\n",
    "\n",
    "    fs.copy_files(\"registry.opendata.aws/roda/ndjson/index.ndjson\",\n",
    "    \"file:///{}/index_copy.ndjson\".format(local_path),\n",
    "    source_filesystem=fs.S3FileSystem())\n",
    "    return hdfs_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at most 2 positional arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m username \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Using libhdfs\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m hdfs \u001b[39m=\u001b[39m HadoopFileSystem(host, port, username, driver\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlibhdfs\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# with hdfs.open('/path/to/file') as f:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# hdfs_alt = HdfsClient(host, port, username, driver='libhdfs3')\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# from hdfs3 import HDFileSystem\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# hdfs = HDFileSystem(host, port, user)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amosd\\OneDrive\\Studium\\Semester4\\BigData_Storage\\mushrooms\\venv\\Lib\\site-packages\\pyarrow\\_hdfs.pyx:67\u001b[0m, in \u001b[0;36mpyarrow._hdfs.HadoopFileSystem.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes at most 2 positional arguments (3 given)"
     ]
    }
   ],
   "source": [
    "from pyarrow import fs\n",
    "\n",
    "host = 'A'\n",
    "port = 'A'\n",
    "user = 'A'\n",
    "\n",
    "\n",
    "\n",
    "hdfs = fs.HadoopFileSystem(host, port, user=user, kerb_ticket=ticket_cache_path)\n",
    "\n",
    "# Use the HadoopFileSystem instance to interact with HDFS\n",
    "with hdfs.open('/path/to/file', 'rb') as f: # https://arrow.apache.org/docs/python/generated/pyarrow.fs.LocalFileSystem.html#pyarrow.fs.LocalFileSystem\n",
    "    data = f.read()\n",
    "    f.write(b'data')\n",
    "\n",
    "# Close the HadoopFileSystem instance\n",
    "hdfs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with hdfs.open('/path/to/file') as f:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
